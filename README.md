

# Character-Level Generatively Pretrained Transformer (GPT)

**Dataset**: William Blake Poems


**Objective**: 
To implement a decoder-only Transformer model (GPT) for character-level text generation, following the “ Attention is All You Need” architecture principles.


**Introduction**:

The goal of this experiment is to make and train a Character-Level Generatively Pretrained Transformer (GPT) which can learn the style and structure of William Blake’s poems. This model will predict the next character based on the sequence of previous characters. By focusing on William Blake’s poetry, we tried to capture the rhythm and rhyme qualities of Blake’s poetic writing. At the same time, we will illustrate our understanding of the Transformer decoder architecture and how we trained it.

The experiment comprises three major stages:
(a) Data Preprocessing
(b) Building the GPT Model
(c) Training and Evaluation


Loss Curve-

<img width="1038" height="814" alt="image" src="https://github.com/user-attachments/assets/63a54794-00b7-4fba-b173-a3f1f66dba97" />


**Results**:

The generated poem follows the stanzaic structure of Balke’s work and properly uses the punctuation and line breaks. Also, it might look like few words are imaginary but they adhere to English phonotactic patterns & this re-affirms our belief that model has internalised stylistic rhythm and the spelling rules.
